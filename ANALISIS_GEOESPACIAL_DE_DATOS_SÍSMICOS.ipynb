{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f0a205",
   "metadata": {},
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1ede0",
   "metadata": {},
   "source": [
    "Generacion de bloques de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar un lote de eventos\n",
    "def process_batch(batch_start, batch_end, earthquake_list, hdf5_events, path_arrays):\n",
    "    print(f'Procesando lote: {batch_start} - {batch_end - 1}')\n",
    "    \n",
    "    X_batch = np.empty((batch_end - batch_start, 6000, 3), dtype=np.float32)\n",
    "    Y_batch = []\n",
    "\n",
    "    for i, event in enumerate(earthquake_list[batch_start:batch_end]):\n",
    "        earthquake_waveform = np.array([hdf5_events.get('data/' + str(event))], dtype=np.float32)\n",
    "        X_batch[i] = earthquake_waveform\n",
    "        Y_batch.append(event)\n",
    "\n",
    "    np.save(path_arrays + f'/array_noise_{batch_start}_{batch_end - 1}.npy', X_batch)\n",
    "    np.save(path_arrays + f'/labels_noise_{batch_start}_{batch_end - 1}.npy', np.array(Y_batch, dtype=np.unicode_))\n",
    "    print(f'Se guardó correctamente labels_noise_{batch_start}_{batch_end - 1} en {path_arrays}')\n",
    "\n",
    "def main():\n",
    "    N = 50000\n",
    "    N_append = 500\n",
    "    path_arrays = \"C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado\"\n",
    "    comienzo = 0\n",
    "    events_hdf5_path = \"C:/Users/Vic_l/Downloads/merge.hdf5\"  # Reemplaza con la ruta correcta\n",
    "    \n",
    "    # Leer lista de eventos (earthquake_list) y otros datos necesarios aquí\n",
    "\n",
    "    with h5py.File(events_hdf5_path, 'r') as hdf5_events:\n",
    "        num_batches = (len(earthquake_list) + N - 1) // N\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch_start = i * N\n",
    "            batch_end = min((i + 1) * N, len(earthquake_list))\n",
    "            process_batch(batch_start, batch_end, earthquake_list, hdf5_events, path_arrays)\n",
    "\n",
    "    print(\"Procesamiento finalizado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bfcf6",
   "metadata": {},
   "source": [
    "Lectura y carga de bloques clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(r\"C:\\Users\\Vic_l\\Documents\\Maestría en Ciencias de la Computación\\Tesis\\dataset_mezclado\\merge_per.csv\")\n",
    "df_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02530032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthquake_signals = 'array_earthquake_200000_249999'\n",
    "#labels_signal = 'labels_earthquake_200000_249999'\n",
    "#x = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals}.npy')\n",
    "#y = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal}.npy')\n",
    "earthquake_signals = ['array_earthquake_0_49999', 'array_noise_0_49999']\n",
    "for i in range(len(earthquake_signals)):\n",
    "  earthquake = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals[i]}.npy')\n",
    "  if i==0:\n",
    "    x = earthquake\n",
    "    y = np.ones(len(earthquake))\n",
    "    print(i)\n",
    "  else:\n",
    "    x = np.concatenate((x, earthquake))\n",
    "    y = np.concatenate((y, np.zeros(len(earthquake))))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x = torch.transpose(x,1,2)\n",
    "y = torch.tensor(y).long()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 128\n",
    "x_train = DataLoader(x_train, batch_size=batch, shuffle=False)\n",
    "y_train = DataLoader(y_train, batch_size=batch, shuffle=False)\n",
    "x_val = DataLoader(x_val, batch_size=batch, shuffle=False)\n",
    "y_val = DataLoader(y_val, batch_size=batch, shuffle=False)\n",
    "x_test = DataLoader(x_test, batch_size=batch, shuffle=False)\n",
    "y_test = DataLoader(y_test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c346a",
   "metadata": {},
   "source": [
    "Modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7227a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=[2, 3], keepdim=True)\n",
    "        var = x.var(dim=[2, 3], keepdim=True)\n",
    "        x = (x - mean) / (torch.sqrt(var + 1e-8))\n",
    "        return x * self.gamma + self.beta\n",
    "\n",
    "# downsample module\n",
    "class Downsample_Module(nn.Module):\n",
    "    def __init__(self, in_chans=3, win_size=7, dsr=2, embed_dim=64):\n",
    "        super(Downsample_Module, self).__init__()\n",
    "        self.emb = nn.Conv2d(in_chans, embed_dim, kernel_size=(win_size, 1), stride=(dsr, 1), padding=(win_size // 2, 0))\n",
    "        self.bn = LayerNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "# attention module\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=dim)\n",
    "        self.conv_sequential = nn.Conv2d(dim, dim, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=dim)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        iden = x.clone()\n",
    "        attn = self.conv2(self.conv_sequential(self.conv1(x)))\n",
    "        return iden * attn\n",
    "\n",
    "class SAN_Block(nn.Module):\n",
    "    def __init__(self, dim, cff_hid=None, cff_out=None):\n",
    "        super(SAN_Block, self).__init__()\n",
    "        self.proj_1 = nn.Conv2d(dim, dim, kernel_size=(1, 1))\n",
    "        self.bn1 = LayerNorm2d(dim)\n",
    "        self.act1 = nn.GELU(approximate='tanh')\n",
    "\n",
    "        self.attn = Attention_Module(dim)\n",
    "\n",
    "        self.proj_2 = nn.Conv2d(dim, dim, kernel_size=(1, 1))\n",
    "        self.bn2 = LayerNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        iden = x.clone()\n",
    "        x = self.act1(self.bn1(self.proj_1(x)))\n",
    "        x = self.attn(x)\n",
    "        x = self.bn2(self.proj_2(x))\n",
    "        return iden + x\n",
    "\n",
    "class SAN(nn.Module):\n",
    "    def __init__(self, stage_blocks=[2, 2, 2, 2], downsampling_dims=[8, 16, 32, 64], down_strides=[4, 3, 2, 2],\n",
    "                 upsampling_dims=[32, 16, 8, 1]):\n",
    "        super(SAN, self).__init__()\n",
    "        self.num_stages = len(stage_blocks)\n",
    "        for i in range(self.num_stages):\n",
    "            down = Downsample_Module(\n",
    "                in_chans=3 if i == 0 else downsampling_dims[i - 1],\n",
    "                win_size=7,\n",
    "                dsr=down_strides[i],\n",
    "                embed_dim=downsampling_dims[i]\n",
    "            )\n",
    "            blocks = nn.ModuleList(\n",
    "                [SAN_Block(downsampling_dims[i], cff_hid=downsampling_dims[i] * 4) for _ in range(stage_blocks[i])]\n",
    "            )\n",
    "            norm = LayerNorm2d(downsampling_dims[i])\n",
    "            setattr(self, f\"down_{i + 1}\", down)\n",
    "            setattr(self, f\"blocks_{i + 1}\", blocks)\n",
    "            setattr(self, f\"norm_{i + 1}\", norm)\n",
    "            \n",
    "        self.lstm = nn.LSTM(6000, hidden_size=100, num_layers=3, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        feats = []\n",
    "        for i in range(self.num_stages):\n",
    "            win_embed = getattr(self, f\"down_{i + 1}\")\n",
    "            blocks = getattr(self, f\"blocks_{i + 1}\")\n",
    "            norm = getattr(self, f\"norm_{i + 1}\")\n",
    "            x = win_embed(x)\n",
    "            for block in blocks:\n",
    "                x = block(x)\n",
    "            x = norm(x)\n",
    "            feats.append(x)\n",
    "        return feats\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "        feats = self.forward_features(x)\n",
    "        pred = feats[-1]\n",
    "        #aplanamiento\n",
    "        output = pred.view(pred.shape[0], -1)\n",
    "        \n",
    "         # Salida de la LSTM\n",
    "        lstm_output, _ = self.lstm(output)\n",
    "        \n",
    "        # Pasar la salida a través de la MLP\n",
    "        mlp_output = self.sigmoid(self.mlp(lstm_output))\n",
    "        return mlp_output.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b192d5",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = SAN(stage_blocks = [4,4,4,4], downsampling_dims = [12, 24, 36, 48], upsampling_dims = [36, 24, 12, 1])\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "train_error = []\n",
    "val_error = []\n",
    "\n",
    "history = {\"train\": {\"loss\": [], \"acc\": []}, \"val\": {\"loss\": [], \"acc\": []}}\n",
    "positive = 1 #terremotos\n",
    "for epoch in range(0, epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    positive = 0\n",
    "    TP_TRAIN = 0\n",
    "    TN_TRAIN = 0\n",
    "    FP_TRAIN = 0\n",
    "    FN_TRAIN = 0\n",
    "    model.train()\n",
    "    for b, (inputs, labels) in enumerate(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.round(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TRAIN += sum([1 if (i==j and i==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FP_TRAIN += sum([1 if (i==0 and j==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FN_TRAIN += sum([1 if (i==1 and j==0) else 0 for i, j in zip(labels, predicted)])\n",
    "            TN_TRAIN += sum([1 if (i==j and i==0) else 0 for i, j in zip(labels, predicted)])\n",
    "\n",
    "             # Calculate the loss\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            train_loss += loss\n",
    "            # Backward pass through the model and update the parameters\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()  # Liberar memoria GPU\n",
    "            optimizer.step()\n",
    "            g=b\n",
    "    epoch_train_loss = train_loss/(g+1)\n",
    "    epoch_train_correct = correct\n",
    "    epoch_train_total = total\n",
    "    precision_train = TP_TRAIN/(TP_TRAIN+FP_TRAIN+0.001)\n",
    "    recall_train = TP_TRAIN/(TP_TRAIN+FN_TRAIN+0.001)\n",
    "    f1_train = TP_TRAIN/(TP_TRAIN+0.5*(FP_TRAIN + FN_TRAIN+0.001))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        for b, (inputs, labels) in enumerate(zip(x_val,y_val)):\n",
    "            predicted_outputs = model(inputs)\n",
    "            predicted = torch.round(predicted_outputs)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i==j and i==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FP_TEST += sum([1 if (i==0 and j==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FN_TEST += sum([1 if (i==1 and j==0) else 0 for i, j in zip(labels, predicted)])\n",
    "            TN_TEST += sum([1 if (i==j and i==0) else 0 for i, j in zip(labels, predicted)])\n",
    "\n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels.float())\n",
    "            val_loss += test_loss\n",
    "            g=b\n",
    "        epoch_val_loss = val_loss/(g+1)\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST+0.001)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST+0.001)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST+0.001))\n",
    "\n",
    "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
    "    #history[\"train\"][\"acc\"].append(epoch_train_error)\n",
    "    history[\"val\"][\"loss\"].append(epoch_val_loss)\n",
    "    #history[\"val\"][\"acc\"].append(epoch_val_error)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(f'Epoch: {epoch+1:03}/{epochs} | Time: {elapsed_time:.4f}s | Train loss: {epoch_train_loss:.4f} | Exactitud {epoch_train_correct, epoch_train_total}| Precision: {precision_train} | Recall: {recall_train} | F1: {f1_train} | Dev loss: {epoch_val_loss:.4f} | Exactitud {epoch_val_correct, epoch_val_total}| Precision: {precision_test} | Recall: {recall_test} | F1: {f1_test} |')\n",
    "    train_error.append(history[\"train\"][\"loss\"][-1])\n",
    "    val_error.append(history[\"val\"][\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP_TRAIN, TN_TRAIN, FP_TRAIN, FN_TRAIN), (TP_TEST, TN_TEST, FP_TEST, FN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "label = []\n",
    "with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        media = torch.tensor([])\n",
    "        for b, (inputs, labels) in enumerate(zip(x_test,y_test)):\n",
    "            predicted_outputs = model(inputs)\n",
    "            predicted = torch.round(predicted_outputs)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i==j and i==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FP_TEST += sum([1 if (i==0 and j==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FN_TEST += sum([1 if (i==1 and j==0) else 0 for i, j in zip(labels, predicted)])\n",
    "            TN_TEST += sum([1 if (i==j and i==0) else 0 for i, j in zip(labels, predicted)])\n",
    "\n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels.float())\n",
    "            val_loss += test_loss\n",
    "            g=b\n",
    "        epoch_val_loss = val_loss/(g+1)\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST+0.001)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST+0.001)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST+0.001))\n",
    "\n",
    "print(epoch_val_correct,  epoch_val_total, precision_test, recall_test, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP_TEST, TN_TEST, FP_TEST, FN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0350cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"clasificador\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48f2d",
   "metadata": {},
   "source": [
    "Reentrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca40baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthquake_signals = 'array_earthquake_200000_249999'\n",
    "#labels_signal = 'labels_earthquake_200000_249999'\n",
    "#x = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals}.npy')\n",
    "#y = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal}.npy')\n",
    "earthquake_signals = ['array_earthquake_50000_99999', 'array_noise_50000_99999']\n",
    "for i in range(len(earthquake_signals)):\n",
    "  earthquake = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals[i]}.npy')\n",
    "  if i==0:\n",
    "    x = earthquake\n",
    "    y = np.ones(len(earthquake))\n",
    "    print(i)\n",
    "  else:\n",
    "    x = np.concatenate((x, earthquake))\n",
    "    y = np.concatenate((y, np.zeros(len(earthquake))))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_elegidos = df_event[df_event['trace_name'].isin(y)]\n",
    "eventos_elegidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x = torch.transpose(x,1,2)\n",
    "y = torch.tensor(y).long()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b799af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca927c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 128\n",
    "x_train = DataLoader(x_train, batch_size=batch, shuffle=False)\n",
    "y_train = DataLoader(y_train, batch_size=batch, shuffle=False)\n",
    "x_val = DataLoader(x_val, batch_size=batch, shuffle=False)\n",
    "y_val = DataLoader(y_val, batch_size=batch, shuffle=False)\n",
    "x_test = DataLoader(x_test, batch_size=batch, shuffle=False)\n",
    "y_test = DataLoader(y_test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b169082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = SAN(stage_blocks = [4,4,4,4], downsampling_dims = [12, 24, 36, 48], upsampling_dims = [36, 24, 12, 1])\n",
    "\n",
    "# Cargar los pesos del modelo guardado\n",
    "checkpoint = torch.load(r\"C:\\Users\\Vic_l\\Documents\\Maestría en Ciencias de la Computación\\Tesis\\clasificador\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "train_error = []\n",
    "val_error = []\n",
    "\n",
    "history = {\"train\": {\"loss\": [], \"acc\": []}, \"val\": {\"loss\": [], \"acc\": []}}\n",
    "positive = 1 #terremotos\n",
    "for epoch in range(0, epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    positive = 0\n",
    "    TP_TRAIN = 0\n",
    "    TN_TRAIN = 0\n",
    "    FP_TRAIN = 0\n",
    "    FN_TRAIN = 0\n",
    "    model.train()\n",
    "    for b, (inputs, labels) in enumerate(zip(x_train,y_train)):\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.round(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TRAIN += sum([1 if (i==j and i==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FP_TRAIN += sum([1 if (i==0 and j==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FN_TRAIN += sum([1 if (i==1 and j==0) else 0 for i, j in zip(labels, predicted)])\n",
    "            TN_TRAIN += sum([1 if (i==j and i==0) else 0 for i, j in zip(labels, predicted)])\n",
    "\n",
    "             # Calculate the loss\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            train_loss += loss\n",
    "            # Backward pass through the model and update the parameters\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()  # Liberar memoria GPU\n",
    "            optimizer.step()\n",
    "            g=b\n",
    "    epoch_train_loss = train_loss/(g+1)\n",
    "    epoch_train_correct = correct\n",
    "    epoch_train_total = total\n",
    "    precision_train = TP_TRAIN/(TP_TRAIN+FP_TRAIN+0.001)\n",
    "    recall_train = TP_TRAIN/(TP_TRAIN+FN_TRAIN+0.001)\n",
    "    f1_train = TP_TRAIN/(TP_TRAIN+0.5*(FP_TRAIN + FN_TRAIN+0.001))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        for b, (inputs, labels) in enumerate(zip(x_val,y_val)):\n",
    "            predicted_outputs = model(inputs)\n",
    "            predicted = torch.round(predicted_outputs)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i==j and i==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FP_TEST += sum([1 if (i==0 and j==1) else 0 for i, j in zip(labels, predicted)])\n",
    "            FN_TEST += sum([1 if (i==1 and j==0) else 0 for i, j in zip(labels, predicted)])\n",
    "            TN_TEST += sum([1 if (i==j and i==0) else 0 for i, j in zip(labels, predicted)])\n",
    "\n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels.float())\n",
    "            val_loss += test_loss\n",
    "            g=b\n",
    "        epoch_val_loss = val_loss/(g+1)\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST+0.001)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST+0.001)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST+0.001))\n",
    "\n",
    "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
    "    #history[\"train\"][\"acc\"].append(epoch_train_error)\n",
    "    history[\"val\"][\"loss\"].append(epoch_val_loss)\n",
    "    #history[\"val\"][\"acc\"].append(epoch_val_error)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(f'Epoch: {epoch+1:03}/{epochs} | Time: {elapsed_time:.4f}s | Train loss: {epoch_train_loss:.4f} | Exactitud {epoch_train_correct, epoch_train_total}| Precision: {precision_train} | Recall: {recall_train} | F1: {f1_train} | Dev loss: {epoch_val_loss:.4f} | Exactitud {epoch_val_correct, epoch_val_total}| Precision: {precision_test} | Recall: {recall_test} | F1: {f1_test} |')\n",
    "    train_error.append(history[\"train\"][\"loss\"][-1])\n",
    "    val_error.append(history[\"val\"][\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"clasificador_reentrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044b602",
   "metadata": {},
   "source": [
    "Lectura y carga de bloques para el modelo de deteccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f770e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthquake_signals = 'array_earthquake_200000_249999'\n",
    "#labels_signal = 'labels_earthquake_200000_249999'\n",
    "#x = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals}.npy')\n",
    "#y = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal}.npy')\n",
    "earthquake_signals = ['array_earthquake_0_49999', 'array_earthquake_50000_99999']\n",
    "labels_signal = ['labels_earthquake_0_49999', 'labels_earthquake_50000_99999']\n",
    "for i in range(len(earthquake_signals)):\n",
    "  earthquake = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals[i]}.npy')\n",
    "  label = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal[i]}.npy')\n",
    "  if i==0:\n",
    "    x = earthquake\n",
    "    y = label\n",
    "    print(i)\n",
    "  else:\n",
    "    x = np.concatenate((x, earthquake))\n",
    "    y = np.concatenate((y, label))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673783cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_elegidos = df_event[df_event['trace_name'].isin(y)]\n",
    "eventos_elegidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wave = eventos_elegidos['p_arrival_sample'].to_numpy()\n",
    "s_wave = eventos_elegidos['s_arrival_sample'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf215d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x = torch.transpose(x,1,2)\n",
    "y = torch.tensor(p_wave).long()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06573ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 128\n",
    "x_train = DataLoader(x_train, batch_size=batch, shuffle=False)\n",
    "y_train = DataLoader(y_train, batch_size=batch, shuffle=False)\n",
    "x_val = DataLoader(x_val, batch_size=batch, shuffle=False)\n",
    "y_val = DataLoader(y_val, batch_size=batch, shuffle=False)\n",
    "x_test = DataLoader(x_test, batch_size=batch, shuffle=False)\n",
    "y_test = DataLoader(y_test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca91b79",
   "metadata": {},
   "source": [
    "Modelo de deteccion de ondas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2311ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=[2, 3], keepdim=True)\n",
    "        var = x.var(dim=[2, 3], keepdim=True)\n",
    "        x = (x - mean) / (torch.sqrt(var + 1e-8))\n",
    "        return x * self.gamma + self.beta\n",
    "\n",
    "# downsample module\n",
    "class Downsample_Module(nn.Module):\n",
    "    def __init__(self, in_chans=3, win_size=7, dsr=2, embed_dim=64):\n",
    "        super(Downsample_Module, self).__init__()\n",
    "        self.emb = nn.Conv2d(in_chans, embed_dim, kernel_size=(win_size, 1), stride=(dsr, 1), padding=(win_size // 2, 0))\n",
    "        self.bn = LayerNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        return self.bn(x)\n",
    "\n",
    "# attention module\n",
    "class Attention_Module(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=dim)\n",
    "        self.conv_sequential = nn.Conv2d(dim, dim, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=dim)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        iden = x.clone()\n",
    "        attn = self.conv2(self.conv_sequential(self.conv1(x)))\n",
    "        return iden * attn\n",
    "\n",
    "# upsampling module\n",
    "class UPsample_Module(nn.Module):\n",
    "    def __init__(self, dim, out_dim, k_size=7, stride=(2, 1), last=False):\n",
    "        super(UPsample_Module, self).__init__()\n",
    "        self.last = last\n",
    "        if last == 2:\n",
    "            self.up = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(dim, dim, kernel_size=(k_size, 1), stride=(stride,1), padding=((k_size - stride + 1)//2, 0),\n",
    "                                       groups=1, output_padding=(0,0)),\n",
    "                    nn.Conv2d(dim, out_dim, kernel_size=(1, 1)),\n",
    "                    LayerNorm2d(out_dim),\n",
    "                    nn.GELU(approximate='tanh')\n",
    "                ) \n",
    "        elif self.last == 3:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.ConvTranspose2d(dim, dim, kernel_size=(k_size, 1), stride=(stride,1), padding=((k_size - stride + 1)//2, 0),\n",
    "                                   groups=1, output_padding=(1,0)),\n",
    "                nn.Conv2d(dim, out_dim, kernel_size=(1, 1)),\n",
    "                nn.ReLU()\n",
    "            )   \n",
    "        else:\n",
    "            self.up = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(dim, dim, kernel_size=(k_size, 1), stride=(stride,1), padding=((k_size - stride + 1)//2, 0),\n",
    "                                       groups=1, output_padding=(1,0)),\n",
    "                    nn.Conv2d(dim, out_dim, kernel_size=(1, 1)),\n",
    "                    LayerNorm2d(out_dim),\n",
    "                    nn.GELU(approximate='tanh')\n",
    "                )  \n",
    "            \n",
    "    def forward(self, x, cat_x=None):\n",
    "        x = self.up(x)\n",
    "        if cat_x is not None:\n",
    "            x = torch.cat([x, cat_x], dim=1)\n",
    "        return x\n",
    "\n",
    "class SAN_Block(nn.Module):\n",
    "    def __init__(self, dim, cff_hid=None, cff_out=None):\n",
    "        super(SAN_Block, self).__init__()\n",
    "        self.proj_1 = nn.Conv2d(dim, dim, kernel_size=(1, 1))\n",
    "        self.bn1 = LayerNorm2d(dim)\n",
    "        self.act1 = nn.GELU(approximate='tanh')\n",
    "\n",
    "        self.attn = Attention_Module(dim)\n",
    "\n",
    "        self.proj_2 = nn.Conv2d(dim, dim, kernel_size=(1, 1))\n",
    "        self.bn2 = LayerNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        iden = x.clone()\n",
    "        x = self.act1(self.bn1(self.proj_1(x)))\n",
    "        x = self.attn(x)\n",
    "        x = self.bn2(self.proj_2(x))\n",
    "        return iden + x\n",
    "\n",
    "class SAN(nn.Module):\n",
    "    def __init__(self, stage_blocks=[2, 2, 2, 2], downsampling_dims=[8, 16, 32, 64], down_strides=[4, 3, 2, 2],\n",
    "                 upsampling_dims=[32, 16, 8, 1]):\n",
    "        super(SAN, self).__init__()\n",
    "        self.num_stages = len(stage_blocks)\n",
    "        for i in range(self.num_stages):\n",
    "            down = Downsample_Module(\n",
    "                in_chans=3 if i == 0 else downsampling_dims[i - 1],\n",
    "                win_size=7,\n",
    "                dsr=down_strides[i],\n",
    "                embed_dim=downsampling_dims[i]\n",
    "            )\n",
    "            blocks = nn.ModuleList(\n",
    "                [SAN_Block(downsampling_dims[i], cff_hid=downsampling_dims[i] * 4) for _ in range(stage_blocks[i])]\n",
    "            )\n",
    "            norm = LayerNorm2d(downsampling_dims[i])\n",
    "            setattr(self, f\"down_{i + 1}\", down)\n",
    "            setattr(self, f\"blocks_{i + 1}\", blocks)\n",
    "            setattr(self, f\"norm_{i + 1}\", norm)\n",
    "\n",
    "        for task in ['p']:\n",
    "            cur_dim = downsampling_dims[-1]\n",
    "            for j in range(self.num_stages - 1):\n",
    "                out_dim = upsampling_dims[j]\n",
    "                cat_dim = downsampling_dims[-j - 2]\n",
    "                setattr(self, f'{task}_up_{j + 1}', UPsample_Module(cur_dim, out_dim, stride=down_strides[-j - 1], last=j))\n",
    "                cur_dim = out_dim + cat_dim\n",
    "            setattr(self, f'{task}_up_{self.num_stages}', UPsample_Module(cur_dim, upsampling_dims[-1], stride=down_strides[0], last=3))\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        feats = []\n",
    "        for i in range(self.num_stages):\n",
    "            win_embed = getattr(self, f\"down_{i + 1}\")\n",
    "            blocks = getattr(self, f\"blocks_{i + 1}\")\n",
    "            norm = getattr(self, f\"norm_{i + 1}\")\n",
    "            x = win_embed(x)\n",
    "            for block in blocks:\n",
    "                x = block(x)\n",
    "            x = norm(x)\n",
    "            feats.append(x)\n",
    "        return feats\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "        feats = self.forward_features(x)\n",
    "        pred = feats[-1]\n",
    "        for j in range(self.num_stages - 1):\n",
    "            pred = getattr(self, f'p_up_{j + 1}')(pred, feats[-j - 2])\n",
    "        pred = getattr(self, f'p_up_{self.num_stages}')(pred)\n",
    "        return pred.view((x.shape[0],6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e03a1",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo de deteccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf153b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = SAN(stage_blocks = [4,4,4,4], downsampling_dims = [12, 24, 36, 48], upsampling_dims = [36, 24, 12, 1])\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "softmax = nn.Softmax(dim=1)\n",
    "train_error = []\n",
    "val_error = []\n",
    "\n",
    "history = {\"train\": {\"loss\": [], \"acc\": []}, \"val\": {\"loss\": [], \"acc\": []}}\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    positive = 0\n",
    "    TP_TRAIN = 0\n",
    "    TN_TRAIN = 0\n",
    "    FP_TRAIN = 0\n",
    "    FN_TRAIN = 0\n",
    "    model.train()\n",
    "    for b, (inputs, labels) in enumerate(zip(x_train,y_train)):\n",
    "            b+=1\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            value, predicted = torch.max(softmax(outputs), 1)\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TRAIN += sum([1 if (i>=0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TRAIN += sum([1 if (i>=0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TRAIN += sum([1 if (i<0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TRAIN += sum([1 if (i<0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "\n",
    "             # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss\n",
    "            # Backward pass through the model and update the parameters\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()  # Liberar memoria GPU\n",
    "            optimizer.step()\n",
    "    epoch_train_loss = train_loss/b\n",
    "    epoch_train_error = error/total\n",
    "    epoch_train_correct = correct\n",
    "    epoch_train_total = total\n",
    "    precision_train = TP_TRAIN/(TP_TRAIN+FP_TRAIN+0.001)\n",
    "    recall_train = TP_TRAIN/(TP_TRAIN+FN_TRAIN+0.001)\n",
    "    f1_train = TP_TRAIN/(TP_TRAIN+0.5*(FP_TRAIN + FN_TRAIN+0.001))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        for b, (inputs, labels) in enumerate(zip(x_val,y_val)):\n",
    "            b+=1\n",
    "            predicted_outputs = model(inputs)\n",
    "            value, predicted = torch.max(softmax(predicted_outputs), 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i>=0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TEST += sum([1 if (i>=0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TEST += sum([1 if (i<0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TEST += sum([1 if (i<0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "\n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels)\n",
    "            val_loss += test_loss\n",
    "        epoch_val_loss = val_loss/b\n",
    "        epoch_val_error = error/total\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST+0.001)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST+0.001)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST+0.001))\n",
    "\n",
    "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
    "    history[\"train\"][\"acc\"].append(epoch_train_error)\n",
    "    history[\"val\"][\"loss\"].append(epoch_val_loss)\n",
    "    history[\"val\"][\"acc\"].append(epoch_val_error)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(f'Epoch: {epoch+1:03}/{epochs} | Time: {elapsed_time:.4f}s | Train loss: {epoch_train_loss:.4f} | Train error: {epoch_train_error:.4f} | Exactitud {epoch_train_correct, epoch_train_total}| Precision: {precision_train} | Recall: {recall_train} | F1: {f1_train} | Dev loss: {epoch_val_loss:.4f} | val error: {epoch_val_error:.4f} | Exactitud {epoch_val_correct, epoch_val_total}| Precision: {precision_test} | Recall: {recall_test} | F1: {f1_test} |')\n",
    "    train_error.append(history[\"train\"][\"loss\"][-1])\n",
    "    val_error.append(history[\"val\"][\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP_TRAIN, TN_TRAIN, FP_TRAIN, FN_TRAIN), (TP_TEST, TN_TEST, FP_TEST, FN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "label = []\n",
    "with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        media = torch.tensor([])\n",
    "        for b, (inputs, labels) in enumerate(zip(x_test,y_test)):\n",
    "            b+=1\n",
    "            predicted_outputs = model(inputs)\n",
    "            value, predicted = torch.max(softmax(predicted_outputs), 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            media = torch.cat((media, torch.abs(predicted-labels).float()),0)\n",
    "            \n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i>=0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TEST += sum([1 if (i>=0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TEST += sum([1 if (i<0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TEST += sum([1 if (i<0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            \n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels)\n",
    "            val_loss += test_loss\n",
    "        epoch_val_loss = val_loss/b\n",
    "        epoch_val_error = error/total\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST))\n",
    "\n",
    "print(epoch_val_error, epoch_val_correct,  epoch_val_total, precision_test, recall_test, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = []\n",
    "for i in media:\n",
    "  if i <= 50:\n",
    "    valores.append(i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = np.array(valores)\n",
    "valores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(valores), np.std(valores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd79c8",
   "metadata": {},
   "source": [
    "Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "# Create Plot\n",
    "ventana = 100\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Time samples')\n",
    "ax1.set_ylabel('Output', color = 'red')\n",
    "a = predicted_outputs[i].detach().numpy()[labels[i]-ventana:labels[i]+ventana]\n",
    "ax1.plot(np.arange(labels[i]-ventana, labels[i]+ventana),a , color = 'red')\n",
    "ax1.tick_params(axis ='y', labelcolor = 'red')\n",
    "\n",
    "# Adding Twin Axes\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Amplitude', color = 'blue')\n",
    "ax2.plot(np.arange(labels[i]-ventana, labels[i]+ventana), inputs[i][0].detach().numpy()[labels[i]-ventana:labels[i]+ventana], color = 'blue')\n",
    "ax2.tick_params(axis ='y', labelcolor = 'blue')\n",
    "\n",
    "# Show plot\n",
    "plt.axvline(labels[i], color='blue', linestyle='--', label='Línea Vertical')\n",
    "plt.axvline(predicted[i], color='red', linestyle='--', label='Línea Vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'detector_onda_p_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec9670",
   "metadata": {},
   "source": [
    "Reentrenamiento del modelo de deteccion de fases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthquake_signals = 'array_earthquake_200000_249999'\n",
    "#labels_signal = 'labels_earthquake_200000_249999'\n",
    "#x = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals}.npy')\n",
    "#y = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal}.npy')\n",
    "earthquake_signals = ['array_earthquake_200000_249999', 'array_earthquake_250000_299999']\n",
    "labels_signal = ['labels_earthquake_200000_249999', 'labels_earthquake_250000_299999']\n",
    "for i in range(len(earthquake_signals)):\n",
    "  earthquake = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{earthquake_signals[i]}.npy')\n",
    "  label = np.load(f'C:/Users/Vic_l/Documents/Maestría en Ciencias de la Computación/Tesis/dataset_mezclado/{labels_signal[i]}.npy')\n",
    "  if i==0:\n",
    "    x = earthquake\n",
    "    y = label\n",
    "    print(i)\n",
    "  else:\n",
    "    x = np.concatenate((x, earthquake))\n",
    "    y = np.concatenate((y, label))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27202f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_elegidos = df_event[df_event['trace_name'].isin(y)]\n",
    "eventos_elegidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad447d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wave = eventos_elegidos['p_arrival_sample'].to_numpy()\n",
    "s_wave = eventos_elegidos['s_arrival_sample'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x = torch.transpose(x,1,2)\n",
    "y = torch.tensor(p_wave).long()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 128\n",
    "x_train = DataLoader(x_train, batch_size=batch, shuffle=False)\n",
    "y_train = DataLoader(y_train, batch_size=batch, shuffle=False)\n",
    "x_val = DataLoader(x_val, batch_size=batch, shuffle=False)\n",
    "y_val = DataLoader(y_val, batch_size=batch, shuffle=False)\n",
    "x_test = DataLoader(x_test, batch_size=batch, shuffle=False)\n",
    "y_test = DataLoader(y_test, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef264686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo\n",
    "model = SAN(stage_blocks = [4,4,4,4], downsampling_dims = [12, 24, 36, 48], upsampling_dims = [36, 24, 12, 1])\n",
    "\n",
    "# Cargar los pesos del modelo guardado\n",
    "checkpoint = torch.load(\"detector_onda_p_1.pt\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "import time\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "epochs = 10\n",
    "softmax = nn.Softmax(dim=1)\n",
    "train_error = []\n",
    "val_error = []\n",
    "\n",
    "history = {\"train\": {\"loss\": [], \"acc\": []}, \"val\": {\"loss\": [], \"acc\": []}}\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    positive = 0\n",
    "    TP_TRAIN = 0\n",
    "    TN_TRAIN = 0\n",
    "    FP_TRAIN = 0\n",
    "    FN_TRAIN = 0\n",
    "    model.train()\n",
    "    for b, (inputs, labels) in enumerate(zip(x_train,y_train)):\n",
    "            b+=1\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            value, predicted = torch.max(softmax(outputs), 1)\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TRAIN += sum([1 if (i>=0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TRAIN += sum([1 if (i>=0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TRAIN += sum([1 if (i<0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TRAIN += sum([1 if (i<0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "\n",
    "             # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss\n",
    "            # Backward pass through the model and update the parameters\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()  # Liberar memoria GPU\n",
    "            optimizer.step()\n",
    "    epoch_train_loss = train_loss/b\n",
    "    epoch_train_error = error/total\n",
    "    epoch_train_correct = correct\n",
    "    epoch_train_total = total\n",
    "    precision_train = TP_TRAIN/(TP_TRAIN+FP_TRAIN)\n",
    "    recall_train = TP_TRAIN/(TP_TRAIN+FN_TRAIN)\n",
    "    f1_train = TP_TRAIN/(TP_TRAIN+0.5*(FP_TRAIN + FN_TRAIN))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        for b, (inputs, labels) in enumerate(zip(x_val,y_val)):\n",
    "            b+=1\n",
    "            predicted_outputs = model(inputs)\n",
    "            value, predicted = torch.max(softmax(predicted_outputs), 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i>=0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TEST += sum([1 if (i>=0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TEST += sum([1 if (i<0.05 and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TEST += sum([1 if (i<0.05 and j>10) else 0 for i, j in zip(value, delta)])\n",
    "\n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels)\n",
    "            val_loss += test_loss\n",
    "        epoch_val_loss = val_loss/b\n",
    "        epoch_val_error = error/total\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST))\n",
    "\n",
    "    history[\"train\"][\"loss\"].append(epoch_train_loss)\n",
    "    history[\"train\"][\"acc\"].append(epoch_train_error)\n",
    "    history[\"val\"][\"loss\"].append(epoch_val_loss)\n",
    "    history[\"val\"][\"acc\"].append(epoch_val_error)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(f'Epoch: {epoch+1:03}/{epochs} | Time: {elapsed_time:.4f}s | Train loss: {epoch_train_loss:.4f} | Train error: {epoch_train_error:.4f} | Exactitud {epoch_train_correct, epoch_train_total}| Precision: {precision_train} | Recall: {recall_train} | F1: {f1_train} | Dev loss: {epoch_val_loss:.4f} | val error: {epoch_val_error:.4f} | Exactitud {epoch_val_correct, epoch_val_total}| Precision: {precision_test} | Recall: {recall_test} | F1: {f1_test} |')\n",
    "    train_error.append(history[\"train\"][\"loss\"][-1])\n",
    "    val_error.append(history[\"val\"][\"loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'detector_onda_p_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP_TRAIN, TN_TRAIN, FP_TRAIN, FN_TRAIN), (TP_TEST, TN_TEST, FP_TEST, FN_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "label = []\n",
    "threshold=0.05\n",
    "with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        correct=0\n",
    "        total=0\n",
    "        error = 0\n",
    "        positive = 0\n",
    "        TP_TEST = 0\n",
    "        TN_TEST = 0\n",
    "        FP_TEST = 0\n",
    "        FN_TEST = 0\n",
    "        media = torch.tensor([])\n",
    "        for b, (inputs, labels) in enumerate(zip(x_test,y_test)):\n",
    "            b+=1\n",
    "            predicted_outputs = model(inputs)\n",
    "            value, predicted = torch.max(softmax(predicted_outputs), 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            delta = torch.abs(predicted-labels)\n",
    "            error += torch.sum(delta.float()).item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            media = torch.cat((media, torch.abs(predicted-labels).float()),0)\n",
    "            \n",
    "            #Metricas de desempeño:\n",
    "            TP_TEST += sum([1 if (i>=threshold and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            FP_TEST += sum([1 if (i>=threshold and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            FN_TEST += sum([1 if (i<threshold and j<=10) else 0 for i, j in zip(value, delta)])\n",
    "            TN_TEST += sum([1 if (i<threshold and j>10) else 0 for i, j in zip(value, delta)])\n",
    "            \n",
    "            #loss\n",
    "            test_loss = criterion(predicted_outputs, labels)\n",
    "            val_loss += test_loss\n",
    "        epoch_val_loss = val_loss/b\n",
    "        epoch_val_error = error/total\n",
    "        epoch_val_correct = correct\n",
    "        epoch_val_total = total\n",
    "        precision_test = TP_TEST/(TP_TEST+FP_TEST)\n",
    "        recall_test = TP_TEST/(TP_TEST+FN_TEST)\n",
    "        f1_test = TP_TEST/(TP_TEST+0.5*(FP_TEST + FN_TEST))\n",
    "\n",
    "print(epoch_val_error, epoch_val_correct,  epoch_val_total, precision_test, recall_test, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = []\n",
    "for i in media:\n",
    "  if i <= 50:\n",
    "    valores.append(i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7860a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = np.array(valores)\n",
    "valores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c05216",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(valores), np.std(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaacb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs =softmax(predicted_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Apply the default theme\n",
    "sns.set_theme(style=\"white\")\n",
    "i = 4\n",
    "softmax1 = nn.Softmax(dim=0)\n",
    "# Create Plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 3))\n",
    "ax1.set_xlabel('Tiempo')\n",
    "ax1.set_ylabel('Amplitude de la salida', color='black')\n",
    "so = softmax1(predicted_outputs[i])\n",
    "a = so.detach().numpy()\n",
    "ax1.plot(np.arange(0, 6000), a, color='red', label='Predicción')\n",
    "\n",
    "# Adding Twin Axes\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Amplitud de la señal', color='black')\n",
    "ax2.plot(np.arange(0,6000), inputs[i][0].detach().numpy(), color='black', label='Señal',linewidth=0.5)\n",
    "\n",
    "# Vertical lines\n",
    "plt.axvline(labels[i], color='blue', linestyle='--', label='Etiqueta real')\n",
    "plt.axvline(predicted[i], color='red', linestyle='--', label='Etiqueta predicha')\n",
    "\n",
    "# Display legend\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Guardar la figura en un archivo (por ejemplo, en formato PNG)\n",
    "plt.savefig('s_fn.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 4\n",
    "ventana = 100\n",
    "softmax1 = nn.Softmax(dim=0)\n",
    "# Create Plot\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel('Tiempo')\n",
    "ax1.set_ylabel('Amplitude de la salida', color='black')\n",
    "so = softmax1(predicted_outputs[i])\n",
    "a = so.detach().numpy()[labels[i]-ventana:labels[i]+ventana]\n",
    "ax1.plot(np.arange(labels[i]-ventana, labels[i]+ventana), a, color='red', label='Predicción')\n",
    "\n",
    "# Adding Twin Axes\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Amplitud de la señal', color='black')\n",
    "ax2.plot(np.arange(labels[i]-ventana, labels[i]+ventana), inputs[i][0].detach().numpy()[labels[i]-ventana:labels[i]+ventana], color='black', label='Señal')\n",
    "\n",
    "# Vertical lines\n",
    "plt.axvline(labels[i], color='blue', linestyle='--', label='Etiqueta real')\n",
    "plt.axvline(predicted[i], color='red', linestyle='--', label='Etiqueta predicha')\n",
    "\n",
    "# Display legend\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Guardar la figura en un archivo (por ejemplo, en formato PNG)\n",
    "plt.savefig('s_fn_amp.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14645262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ejemplo de valores de la matriz de confusión\n",
    "tp = 77671  # True Positive\n",
    "tn = 4881  # True Negative\n",
    "fp = 17296   # False Positive\n",
    "fn = 3176  # False Negative\n",
    "\n",
    "# Crear las listas de etiquetas reales y predichas\n",
    "y_true = [1] * tp + [0] * tn + [0] * fp + [1] * fn\n",
    "y_pred = [1] * tp + [0] * tn + [1] * fp + [0] * fn\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Mostrar la matriz de confusión con seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Realidad')\n",
    "plt.title('Matriz de Confusión')\n",
    "# Guardar la figura en un archivo (por ejemplo, en formato PNG)\n",
    "plt.savefig('matriz_confusion.png')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
